{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K74zxktiQyub"
   },
   "source": [
    "# **StyleGANXL + CLIP üñºÔ∏è**\n",
    "\n",
    "## Generate images from text prompts using StyleGANXL with CLIP guidance.\n",
    "\n",
    "(Modified by Katherine Crowson to optimize in W+ space)\n",
    "\n",
    "This notebook is a work in progress, head over [here](https://github.com/CasualGANPapers/unconditional-StyleGAN-CLIP) if you want to be up to date with its changes.\n",
    "\n",
    "Largely based on code by  [Katherine Crowson](https://github.com/crowsonkb) and [nshepperd](https://github.com/nshepperd).\n",
    "\n",
    "Mostly made possible because of [StyleGAN-XL](https://github.com/autonomousvision/stylegan_xl) and [CLIP](https://github.com/openai/CLIP).\n",
    "\n",
    "Created by [Eugenio Herrera](https://github.com/ouhenio) and [Rodrigo Mello](https://github.com/ryudrigo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doClone = False\n",
    "doPip = False\n",
    "doCPU = False # << We can do this on CPU but it's a lot slower... (there's a final check for CUDA as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6Ri2kT3N3Gc",
    "outputId": "297c85b3-8638-416b-e394-c6dfe711d51f"
   },
   "outputs": [],
   "source": [
    "if doClone:\n",
    "    !git clone https://github.com/autonomousvision/stylegan_xl\n",
    "    !git clone https://github.com/openai/CLIP\n",
    "    !git clone https://github.com/crowsonkb/esgd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6Ri2kT3N3Gc",
    "outputId": "297c85b3-8638-416b-e394-c6dfe711d51f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doPip:\n",
    "    !pip install -e ./CLIP\n",
    "    !pip install einops ninja\n",
    "    !pip install timm==0.5.4\n",
    "    !pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6Ri2kT3N3Gc",
    "outputId": "297c85b3-8638-416b-e394-c6dfe711d51f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./CLIP')\n",
    "sys.path.append('./stylegan_xl')\n",
    "sys.path.append('./esgd')\n",
    "\n",
    "import io\n",
    "import os, time, glob\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import clip\n",
    "import unicodedata\n",
    "import re\n",
    "from esgd import ESGD\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from IPython.display import display\n",
    "from einops import rearrange\n",
    "#from google.colab import files << No need for this local version\n",
    "import dnnlib\n",
    "import legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6Ri2kT3N3Gc",
    "outputId": "297c85b3-8638-416b-e394-c6dfe711d51f"
   },
   "outputs": [],
   "source": [
    "# Functions \n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "def fetch_model(url_or_path):\n",
    "    !wget -c '{url_or_path}'\n",
    "\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "def norm1(prompt):\n",
    "    \"Normalize to the unit sphere.\"\n",
    "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "def prompts_dist_loss(x, targets, loss):\n",
    "    if len(targets) == 1: # Keeps consitent results vs previous method for single objective guidance\n",
    "      return loss(x, targets[0])\n",
    "    distances = [loss(x, target) for target in targets]\n",
    "    return torch.stack(distances, dim=-1).sum(dim=-1)\n",
    "\n",
    "class MakeCutouts(torch.nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
    "\n",
    "def embed_image(image):\n",
    "  n = image.shape[0]\n",
    "  cutouts = make_cutouts(image)\n",
    "  embeds = clip_model.embed_cutout(cutouts)\n",
    "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
    "  return embeds\n",
    "\n",
    "def embed_url(url):\n",
    "  image = Image.open(fetch(url)).convert('RGB')\n",
    "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
    "\n",
    "#class CLIP(object):\n",
    " # def __init__(self):\n",
    "  #  clip_model = \"ViT-B/16\"\n",
    "   # self.model, _ = clip.load(clip_model)\n",
    "    #self.model = self.model.requires_grad_(False)\n",
    "    #self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                          #std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    " # @torch.no_grad()\n",
    "  #def embed_text(self, prompt):\n",
    "   #   \"Normalized clip text embedding.\"\n",
    "    #  return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
    "#\n",
    " # def embed_cutout(self, image):\n",
    "  #    \"Normalized clip image embedding.\"\n",
    "   #   return norm1(self.model.encode_image(self.normalize(image)))\n",
    "\n",
    "#clip_model = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guZeRGM6OmaU",
    "outputId": "7e73cb1e-912c-47b6-fd54-213afaa2ee4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-22 14:29:26--  https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet1024.pkl\n",
      "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.135.211, 52.219.46.95, 52.219.170.93, ...\n",
      "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.135.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select model\n",
    "Model = 'Imagenet-1024' #@param [\"Imagenet-1024\", \"Imagenet-512\", \"Imagenet-256\", \"Imagenet-128\", \"Pokemon\", \"FFHQ\"]\n",
    "\n",
    "network_url = {\n",
    "    \"Imagenet-1024\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet1024.pkl\",\n",
    "    \"Imagenet-512\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet512.pkl\",\n",
    "    \"Imagenet-256\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet256.pkl\",\n",
    "    \"Imagenet-128\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet128.pkl\",\n",
    "    \"Pokemon-1024\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon1024.pkl\",\n",
    "    \"Pokemon-512\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon512.pkl\",\n",
    "    \"Pokemon-256\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon256.pkl\",\n",
    "    \"FFHQ-256\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/ffhq256.pkl\"\n",
    "}\n",
    "\n",
    "network_name = network_url[Model].split(\"/\")[-1]\n",
    "fetch_model(network_url[Model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "guZeRGM6OmaU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7e73cb1e-912c-47b6-fd54-213afaa2ee4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.conda/envs/neuro/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU or CPU\n",
    "if doCPU:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on CPU\")\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    print('Using device:', device, file=sys.stderr)\n",
    "\n",
    "  \n",
    "# Load network\n",
    "with dnnlib.util.open_url(network_name) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "\n",
    "# Generate latent vectors and conditioning vectors on CPU\n",
    "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
    "cs = torch.zeros([10000, G.mapping.c_dim], device=device)\n",
    "for i in range(cs.shape[0]):\n",
    "    cs[i, i // 10] = 1\n",
    "\n",
    "w_stds = G.mapping(zs, cs)\n",
    "w_stds = w_stds.reshape(10, 1000, G.num_ws, -1)\n",
    "w_stds=w_stds.std(0).mean(0)[0]\n",
    "w_all_classes_avg = G.mapping.w_avg.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzXfMVE1h-fj",
    "outputId": "14b0aae4-d35f-46cb-b1a7-dfa3354b4d24"
   },
   "outputs": [],
   "source": [
    "if doCPU:\n",
    "    print('Only doing this on GPU')\n",
    "else:\n",
    "    tf = Compose([\n",
    "      # Resize(224),\n",
    "      lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
    "    ])\n",
    "\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    t,r,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a loop for image collection\n",
    "# CLASSES\n",
    "class_idx1 = np.arange(0,1000)\n",
    "class_idx2 = np.zeros(class_idx1.shape)\n",
    "\n",
    "#class_idx1 = []\n",
    "#class_idx2 = []\n",
    "\n",
    "# STEPS\n",
    "#steps = np.arange(0,1.1,0.1) # all steps\n",
    "steps = [0] # EXAMPLAR ONLY\n",
    "\n",
    "# SHOW IMAGES?\n",
    "doDisplayImages = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Oz57aGjsRUXf"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for dimension 0 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m zs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m1\u001b[39m, G\u001b[38;5;241m.\u001b[39mmapping\u001b[38;5;241m.\u001b[39mz_dim], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m cs1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, G\u001b[38;5;241m.\u001b[39mmapping\u001b[38;5;241m.\u001b[39mc_dim], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 18\u001b[0m cs1[\u001b[38;5;241m0\u001b[39m][class_idx_1] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Map to the intermediate latent space\u001b[39;00m\n\u001b[1;32m     21\u001b[0m w_stds1 \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mmapping(zs, cs1)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1000 is out of bounds for dimension 0 with size 1000"
     ]
    }
   ],
   "source": [
    "for idx, x in np.ndenumerate(class_idx1):\n",
    "    \n",
    "    # Assume G.mapping.z_dim and G.mapping.c_dim are defined, as well as tf function\n",
    "    # Double-check if cuda is available, otherwise use CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Define the initial and final class indices\n",
    "    class_idx_1 = int(class_idx1[idx])  # Replace with the starting class index\n",
    "    class_idx_2 = int(class_idx2[idx])  # Replace with the target class index\n",
    "    if len(steps) == 1 and steps[0] == 0:\n",
    "        img_lable = 'c' + str(class_idx_1).zfill(4)\n",
    "    else:\n",
    "        img_lable = 'c' + str(class_idx_1).zfill(4) + '-' + str(class_idx_2).zfill(4) \n",
    "    \n",
    "    # Generate the initial latent vector and condition vector\n",
    "    zs = torch.randn([1, G.mapping.z_dim], device=device)\n",
    "    cs1 = torch.zeros([1, G.mapping.c_dim], device=device)\n",
    "    cs1[0][class_idx_1] = 1\n",
    "    \n",
    "    # Map to the intermediate latent space\n",
    "    w_stds1 = G.mapping(zs, cs1)\n",
    "    w_avg1 = G.mapping.w_avg\n",
    "    w_avg1 = w_avg1[cs1[0].bool()]\n",
    "    w_avg1 = w_avg1.unsqueeze(1).repeat(1, G.mapping.num_ws, 1)\n",
    "    w_stds1 = w_avg1\n",
    "    \n",
    "    # Generate the final latent vector and condition vector\n",
    "    zs = torch.randn([1, G.mapping.z_dim], device=device)\n",
    "    cs2 = torch.zeros([1, G.mapping.c_dim], device=device)\n",
    "    cs2[0][class_idx_2] = 1\n",
    "    \n",
    "    # Map to the intermediate latent space\n",
    "    w_stds2 = G.mapping(zs, cs2)\n",
    "    w_avg2 = G.mapping.w_avg\n",
    "    w_avg2 = w_avg2[cs2[0].bool()]\n",
    "    w_avg2 = w_avg2.unsqueeze(1).repeat(1, G.mapping.num_ws, 1)\n",
    "    w_stds2 = w_avg2\n",
    "    \n",
    "    # Generate and display images morphing from class 1 to class 2 in 10 steps\n",
    "    #steps = np.arange(0,1.1,0.1)\n",
    "    #steps = [0,1]\n",
    "    for alpha in steps:\n",
    "        w_interp = (1 - alpha) * w_stds1 + alpha * w_stds2\n",
    "        caio = G.synthesis(w_interp, noise_mode='const')\n",
    "        if doDisplayImages:\n",
    "            display(TF.to_pil_image(tf(caio)[0]))\n",
    "        img_lableidx = img_lable + '_i' + str(round(10*alpha)).zfill(2)\n",
    "        img = TF.to_pil_image(tf(caio)[0])\n",
    "        if len(steps) == 1 and steps[0] == 0:\n",
    "            os.makedirs(os.path.join('imgout', 'examplar'),exist_ok=True)\n",
    "            img.save(os.path.join('imgout', 'examplar', img_lableidx + '.png'))\n",
    "        else:\n",
    "            os.makedirs(os.path.join('imgout', img_lable),exist_ok=True)\n",
    "            img.save(os.path.join('imgout', img_lable, img_lableidx + '.png'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
